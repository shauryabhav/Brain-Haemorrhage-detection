{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5676aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from keras.layers import Conv2D, Flatten, Dense, MaxPool2D, BatchNormalization, GlobalAveragePooling2D\n",
    "#from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "#from keras.preprocessing import image\n",
    "#from keras.models import Sequential\n",
    "#from keras.models import Model\n",
    "#from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.applications.resnet import ResNet101\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342fe5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1051 images belonging to 3 classes.\n",
      "Found 200 images belonging to 3 classes.\n",
      "Found 99 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 08:20:27.908646: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-03 08:20:28.787649: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-05-03 08:20:28.787841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 918 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 1g.5gb, pci bus id: 0000:90:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 08:20:37.651560: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8500\n",
      "2024-05-03 08:20:39.217125: I tensorflow/stream_executor/cuda/cuda_blas.cc:1804] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-05-03 08:20:39.221171: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'binary_crossentropy/logistic_loss/mul' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_73130/201299082.py\", line 102, in <cell line: 95>\n      cnn_trained_model = train_cnn_model(train_generator, valid_generator)\n    File \"/tmp/ipykernel_73130/201299082.py\", line 72, in train_cnn_model\n      model.fit(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 890, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 948, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1930, in binary_crossentropy\n      backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5283, in binary_crossentropy\n      return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\nNode: 'binary_crossentropy/logistic_loss/mul'\nrequired broadcastable shapes\n\t [[{{node binary_crossentropy/logistic_loss/mul}}]] [Op:__inference_train_function_16631]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m valid_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/data/data_dir/model-data/val\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m train_generator, test_generator, valid_generator \u001b[38;5;241m=\u001b[39m preprocess(train_data_dir, test_data_dir, valid_data_dir)\n\u001b[0;32m--> 102\u001b[0m cnn_trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m    105\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m extract_features(cnn_trained_model, train_generator)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtrain_cnn_model\u001b[0;34m(train_generator, valid_generator)\u001b[0m\n\u001b[1;32m     70\u001b[0m model \u001b[38;5;241m=\u001b[39m cnn_model()\n\u001b[1;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'binary_crossentropy/logistic_loss/mul' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_73130/201299082.py\", line 102, in <cell line: 95>\n      cnn_trained_model = train_cnn_model(train_generator, valid_generator)\n    File \"/tmp/ipykernel_73130/201299082.py\", line 72, in train_cnn_model\n      model.fit(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 890, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 948, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1930, in binary_crossentropy\n      backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5283, in binary_crossentropy\n      return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\nNode: 'binary_crossentropy/logistic_loss/mul'\nrequired broadcastable shapes\n\t [[{{node binary_crossentropy/logistic_loss/mul}}]] [Op:__inference_train_function_16631]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Image preprocessing\n",
    "def preprocess(train_data_dir, valid_data_dir, test_data_dir):\n",
    "    img_height, img_width = (224, 224)\n",
    "    batch_size = 32\n",
    "\n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True,\n",
    "                                       validation_split=0.4)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training')\n",
    "\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation')\n",
    "\n",
    "    test_generator = train_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        subset='validation')\n",
    "\n",
    "    return train_generator, test_generator, valid_generator\n",
    "\n",
    "def extract_features(base_model, generator):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(len(generator)):\n",
    "        x, y = generator[i]\n",
    "        feature_batch = base_model.predict(x)\n",
    "        label_batch = np.argmax(y, axis=1)\n",
    "        features.append(feature_batch)\n",
    "        labels.append(label_batch)\n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    return features, labels\n",
    "\n",
    "def cnn_model():\n",
    "    base_model = ResNet101(include_top=False, weights='imagenet')\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)  # Assuming 2 classes for binary classification\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "\n",
    "def train_cnn_model(train_generator, valid_generator):\n",
    "    model = cnn_model()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=valid_generator,\n",
    "        verbose=1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_xgb_model(train_features, train_labels, valid_features, valid_labels):\n",
    "    dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
    "    dvalid = xgb.DMatrix(valid_features, label=valid_labels)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': len(np.unique(train_labels)),\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "\n",
    "    evals = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    num_round = 100\n",
    "    xgb_model = xgb.train(params, dtrain, num_round, evals, early_stopping_rounds=10, verbose_eval=10)\n",
    "    return xgb_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_data_dir = \"/workspace/data/data_dir/model-data/train\"\n",
    "    test_data_dir = \"/workspace/data/data_dir/model-data/test\"\n",
    "    valid_data_dir = \"/workspace/data/data_dir/model-data/val\"\n",
    "\n",
    "    train_generator, test_generator, valid_generator = preprocess(train_data_dir, test_data_dir, valid_data_dir)\n",
    "\n",
    "    cnn_trained_model = train_cnn_model(train_generator, valid_generator)\n",
    "\n",
    "    # Extract features\n",
    "    train_features, train_labels = extract_features(cnn_trained_model, train_generator)\n",
    "    valid_features, valid_labels = extract_features(cnn_trained_model, valid_generator)\n",
    "\n",
    "    # Train XGBoost model\n",
    "    xgb_model = train_xgb_model(train_features, train_labels, valid_features, valid_labels)\n",
    "\n",
    "    # Evaluate XGBoost model\n",
    "    test_features, test_labels = extract_features(cnn_trained_model, test_generator)\n",
    "    dtest = xgb.DMatrix(test_features, label=test_labels)\n",
    "    preds = xgb_model.predict(dtest)\n",
    "    accuracy = np.sum(preds == test_labels) / len(test_labels)\n",
    "    print(\"XGBoost Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5a1ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1051 images belonging to 3 classes.\n",
      "Found 200 images belonging to 3 classes.\n",
      "Found 99 images belonging to 3 classes.\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 2.1237 - accuracy: 0.8230\n",
      "Epoch 1: val_loss improved from inf to 199.40741, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "33/33 [==============================] - 20s 566ms/step - loss: 2.1237 - accuracy: 0.8230 - val_loss: 199.4074 - val_accuracy: 0.8700\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.7128 - accuracy: 0.8411\n",
      "Epoch 2: val_loss improved from 199.40741 to 53.05507, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "33/33 [==============================] - 17s 508ms/step - loss: 0.7128 - accuracy: 0.8411 - val_loss: 53.0551 - val_accuracy: 0.8700\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.8421\n",
      "Epoch 3: val_loss improved from 53.05507 to 3.91335, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "33/33 [==============================] - 17s 530ms/step - loss: 0.4616 - accuracy: 0.8421 - val_loss: 3.9133 - val_accuracy: 0.8700\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8620\n",
      "Epoch 4: val_loss improved from 3.91335 to 1.26365, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "33/33 [==============================] - 18s 531ms/step - loss: 0.3594 - accuracy: 0.8620 - val_loss: 1.2637 - val_accuracy: 0.8650\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.3659 - accuracy: 0.8696\n",
      "Epoch 5: val_loss improved from 1.26365 to 0.45244, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "33/33 [==============================] - 17s 521ms/step - loss: 0.3659 - accuracy: 0.8696 - val_loss: 0.4524 - val_accuracy: 0.8400\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.8687\n",
      "Epoch 6: val_loss did not improve from 0.45244\n",
      "33/33 [==============================] - 16s 486ms/step - loss: 0.3526 - accuracy: 0.8687 - val_loss: 0.4985 - val_accuracy: 0.7400\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.8630\n",
      "Epoch 7: val_loss improved from 0.45244 to 0.33330, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "33/33 [==============================] - 17s 509ms/step - loss: 0.3243 - accuracy: 0.8630 - val_loss: 0.3333 - val_accuracy: 0.8350\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.8582\n",
      "Epoch 8: val_loss did not improve from 0.33330\n",
      "33/33 [==============================] - 16s 475ms/step - loss: 0.3515 - accuracy: 0.8582 - val_loss: 0.3409 - val_accuracy: 0.8400\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8735\n",
      "Epoch 9: val_loss did not improve from 0.33330\n",
      "33/33 [==============================] - 15s 454ms/step - loss: 0.2975 - accuracy: 0.8735 - val_loss: 0.5114 - val_accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.3151 - accuracy: 0.8858\n",
      "Epoch 10: val_loss did not improve from 0.33330\n",
      "33/33 [==============================] - 15s 445ms/step - loss: 0.3151 - accuracy: 0.8858 - val_loss: 0.3667 - val_accuracy: 0.8350\n",
      "33/33 [==============================] - 12s 353ms/step\n",
      "99/99 [==============================] - 2s 15ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [1 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 102>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Apply XGBoost\u001b[39;00m\n\u001b[1;32m    118\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier()\n\u001b[0;32m--> 119\u001b[0m \u001b[43mxgb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m xgb_classifier\u001b[38;5;241m.\u001b[39mscore(test_features, test_labels)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:1357\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m   1356\u001b[0m ):\n\u001b[0;32m-> 1357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1358\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1359\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1360\u001b[0m     )\n\u001b[1;32m   1362\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [1 2]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "def preprocess(train_data_dir, valid_data_dir, test_data_dir):\n",
    "    img_height, img_width = (227, 227)  # Image dimensions assumed for AlexNet\n",
    "    batch_size = 32\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True,\n",
    "                                       validation_split=0.4)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training')\n",
    "\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation')\n",
    "\n",
    "    test_generator = train_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        subset='validation')\n",
    "\n",
    "    return train_generator, test_generator, valid_generator\n",
    "\n",
    "def alexnet_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(96, (11, 11), strides=(4, 4), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(256, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(384, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(384, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_cnn_model(train_generator, valid_generator, num_classes):\n",
    "    base_model = alexnet_model((227, 227, 3), num_classes)\n",
    "    base_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint('/workspace/data/data_dir/saved/best/best_model.h5',\n",
    "                                          monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "    \n",
    "    log_dir = \"/workspace/data/data_dir/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    base_model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=valid_generator,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    return base_model\n",
    "\n",
    "def extract_features(model, generator):\n",
    "    features = model.predict(generator)\n",
    "    labels = generator.labels\n",
    "    return features, labels\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_data_dir = \"/workspace/data/data_dir/model-data/train\"\n",
    "    test_data_dir = \"/workspace/data/data_dir/model-data/test\"\n",
    "    valid_data_dir = \"/workspace/data/data_dir/model-data/val\"\n",
    "\n",
    "    train_generator, test_generator, valid_generator = preprocess(train_data_dir, test_data_dir, valid_data_dir)\n",
    "    \n",
    "    num_classes = train_generator.num_classes\n",
    "    \n",
    "    cnn_trained_model = train_cnn_model(train_generator, valid_generator, num_classes)\n",
    "    \n",
    "    # Extract features\n",
    "    train_features, train_labels = extract_features(cnn_trained_model, train_generator)\n",
    "    test_features, test_labels = extract_features(cnn_trained_model, test_generator)\n",
    "    \n",
    "    # Apply XGBoost\n",
    "    xgb_classifier = xgb.XGBClassifier()\n",
    "    xgb_classifier.fit(train_features, train_labels)\n",
    "    accuracy = xgb_classifier.score(test_features, test_labels)\n",
    "    print(\"XGBoost Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb6dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
