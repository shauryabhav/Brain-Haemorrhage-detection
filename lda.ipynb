{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12809f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from keras.layers import Conv2D, Flatten, Dense, MaxPool2D, BatchNormalization, GlobalAveragePooling2D\n",
    "#from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "#from keras.preprocessing import image\n",
    "#from keras.models import Sequential\n",
    "#from keras.models import Model\n",
    "#from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.applications.resnet import ResNet101\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6feef52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1750 images belonging to 3 classes.\n",
      "Found 249 images belonging to 3 classes.\n",
      "Found 502 images belonging to 3 classes.\n",
      "Number of classes: 3\n",
      "Input shape: (1,)\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1065 - accuracy: 0.0312\n",
      "Epoch 1: val_loss improved from inf to 1.01812, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 1.1065 - accuracy: 0.0312 - val_loss: 1.0181 - val_accuracy: 0.8750\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0400 - accuracy: 0.8438\n",
      "Epoch 2: val_loss improved from 1.01812 to 0.96098, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.0400 - accuracy: 0.8438 - val_loss: 0.9610 - val_accuracy: 0.7812\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9871 - accuracy: 0.8438\n",
      "Epoch 3: val_loss improved from 0.96098 to 0.91842, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.9871 - accuracy: 0.8438 - val_loss: 0.9184 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9428 - accuracy: 0.8438\n",
      "Epoch 4: val_loss improved from 0.91842 to 0.88350, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.9428 - accuracy: 0.8438 - val_loss: 0.8835 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9037 - accuracy: 0.8438\n",
      "Epoch 5: val_loss improved from 0.88350 to 0.85293, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.9037 - accuracy: 0.8438 - val_loss: 0.8529 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8687 - accuracy: 0.8438\n",
      "Epoch 6: val_loss improved from 0.85293 to 0.82492, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.8687 - accuracy: 0.8438 - val_loss: 0.8249 - val_accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8352 - accuracy: 0.8438\n",
      "Epoch 7: val_loss improved from 0.82492 to 0.79926, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.8352 - accuracy: 0.8438 - val_loss: 0.7993 - val_accuracy: 0.7812\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8037 - accuracy: 0.8438\n",
      "Epoch 8: val_loss improved from 0.79926 to 0.77492, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.8037 - accuracy: 0.8438 - val_loss: 0.7749 - val_accuracy: 0.8125\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7738 - accuracy: 0.8438\n",
      "Epoch 9: val_loss improved from 0.77492 to 0.75207, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.7738 - accuracy: 0.8438 - val_loss: 0.7521 - val_accuracy: 0.8125\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7450 - accuracy: 0.8438\n",
      "Epoch 10: val_loss improved from 0.75207 to 0.73101, saving model to /workspace/data/data_dir/saved/best/best_model.h5\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.7450 - accuracy: 0.8438 - val_loss: 0.7310 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "def preprocess(train_data_dir, valid_data_dir, test_data_dir):\n",
    "    img_height, img_width = (227, 227)  # Image dimensions assumed for AlexNet\n",
    "    batch_size = 32\n",
    "\n",
    "    # Data generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    test_generator = train_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # Preprocess images using LDA\n",
    "    lda = LDA()\n",
    "    lda.fit(train_generator[0][0].reshape(-1, img_height * img_width * 3), np.argmax(train_generator[0][1], axis=1))\n",
    "\n",
    "    # Transform image data using LDA\n",
    "    lda_train_data = lda.transform(train_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "    lda_valid_data = lda.transform(valid_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "    lda_test_data = lda.transform(test_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "\n",
    "    # Get the target variables\n",
    "    lda_train_target = np.argmax(train_generator[0][1], axis=1)\n",
    "    lda_valid_target = np.argmax(valid_generator[0][1], axis=1)\n",
    "    lda_test_target = np.argmax(test_generator[0][1], axis=1)\n",
    "\n",
    "    # Create new generators with transformed data and target variables\n",
    "    lda_train_generator = (lda_train_data, train_generator[0][1])\n",
    "    lda_valid_generator = (lda_valid_data, valid_generator[0][1])\n",
    "    lda_test_generator = (lda_test_data, test_generator[0][1])\n",
    "\n",
    "    return lda, lda_train_generator, lda_test_generator, lda_valid_generator\n",
    "\n",
    "def model_trainer(epochs=10):\n",
    "    train_data_dir = \"/workspace/data/data_dir/model-data/train\"\n",
    "    test_data_dir = \"/workspace/data/data_dir/model-data/test\"\n",
    "    valid_data_dir = \"/workspace/data/data_dir/model-data/val\"\n",
    "\n",
    "    lda, lda_train_generator, lda_test_generator, lda_valid_generator = preprocess(\n",
    "        train_data_dir=train_data_dir,\n",
    "        test_data_dir=test_data_dir,\n",
    "        valid_data_dir=valid_data_dir)\n",
    "\n",
    "    num_classes = lda_train_generator[1].shape[1]  # Get number of classes from the generator\n",
    "    print(\"Number of classes:\", num_classes)\n",
    "\n",
    "    input_shape = (lda_train_generator[0].shape[1],)  # Adjust input shape according to LDA components\n",
    "    print(\"Input shape:\", input_shape)\n",
    "\n",
    "    model = lda_model(input_shape, num_classes)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint('/workspace/data/data_dir/saved/best/best_model.h5',\n",
    "                                          monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "    \n",
    "    log_dir = \"/workspace/data/data_dir/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    model.fit(\n",
    "        lda_train_generator[0],\n",
    "        lda_train_generator[1],\n",
    "        epochs=epochs,\n",
    "        validation_data=(lda_valid_generator[0], lda_valid_generator[1]),\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lda_model = model_trainer(epochs=10)\n",
    "    lda_model.save('/workspace/data/data_dir/saved/saved_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ec1d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1750 images belonging to 3 classes.\n",
      "Found 249 images belonging to 3 classes.\n",
      "Found 502 images belonging to 3 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"model_1\" (type Functional).\n\nInput 0 of layer \"flatten_1\" is incompatible with the layer: expected min_ndim=1, found ndim=0. Full shape received: ()\n\nCall arguments received by layer \"model_1\" (type Functional):\n  • inputs=('tf.Tensor(shape=(), dtype=int32)',)\n  • training=3\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 106>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     lda_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/data/data_dir/saved/saved_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mmodel_trainer\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     65\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m lda_train_generator[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Get number of classes from the generator\u001b[39;00m\n\u001b[1;32m     66\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (lda_train_generator[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)  \u001b[38;5;66;03m# Adjust input shape according to LDA components\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlda_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     71\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/data/data_dir/saved/best/best_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     72\u001b[0m                                       monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py:228\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    226\u001b[0m   ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    227\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    230\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    231\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    232\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"model_1\" (type Functional).\n\nInput 0 of layer \"flatten_1\" is incompatible with the layer: expected min_ndim=1, found ndim=0. Full shape received: ()\n\nCall arguments received by layer \"model_1\" (type Functional):\n  • inputs=('tf.Tensor(shape=(), dtype=int32)',)\n  • training=3\n  • mask=None"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "def preprocess(train_data_dir, valid_data_dir, test_data_dir):\n",
    "    img_height, img_width = (227, 227)  # Image dimensions assumed for AlexNet\n",
    "    batch_size = 32\n",
    "\n",
    "    # Data generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    test_generator = train_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # Preprocess images using LDA\n",
    "    lda = LDA()\n",
    "    lda.fit(train_generator[0][0].reshape(-1, img_height * img_width * 3), np.argmax(train_generator[0][1], axis=1))\n",
    "\n",
    "    # Transform image data using LDA\n",
    "    lda_train_data = lda.transform(train_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "    lda_valid_data = lda.transform(valid_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "    lda_test_data = lda.transform(test_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "\n",
    "    # Get the target variables\n",
    "    lda_train_target = np.argmax(train_generator[0][1], axis=1)\n",
    "    lda_valid_target = np.argmax(valid_generator[0][1], axis=1)\n",
    "    lda_test_target = np.argmax(test_generator[0][1], axis=1)\n",
    "\n",
    "    # Create new generators with transformed data and target variables\n",
    "    lda_train_generator = (lda_train_data, train_generator[0][1])\n",
    "    lda_valid_generator = (lda_valid_data, valid_generator[0][1])\n",
    "    lda_test_generator = (lda_test_data, test_generator[0][1])\n",
    "\n",
    "    return lda, lda_train_generator, lda_test_generator, lda_valid_generator\n",
    "\n",
    "def model_trainer(epochs=10):\n",
    "    train_data_dir = \"/workspace/data/data_dir/model-data/train\"\n",
    "    test_data_dir = \"/workspace/data/data_dir/model-data/test\"\n",
    "    valid_data_dir = \"/workspace/data/data_dir/model-data/val\"\n",
    "\n",
    "    lda, lda_train_generator, lda_test_generator, lda_valid_generator = preprocess(\n",
    "        train_data_dir=train_data_dir,\n",
    "        test_data_dir=test_data_dir,\n",
    "        valid_data_dir=valid_data_dir)\n",
    "\n",
    "    num_classes = lda_train_generator[1].shape[1]  # Get number of classes from the generator\n",
    "    input_shape = (lda_train_generator[0].shape[1],)  # Adjust input shape according to LDA components\n",
    "\n",
    "    model = lda_model(input_shape, num_classes)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint('/workspace/data/data_dir/saved/best/best_model.h5',\n",
    "                                          monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "    \n",
    "    log_dir = \"/workspace/data/data_dir/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    history = model.fit(\n",
    "        lda_train_generator[0],\n",
    "        lda_train_generator[1],\n",
    "        epochs=epochs,\n",
    "        validation_data=(lda_valid_generator[0], lda_valid_generator[1]),\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lda_model = model_trainer(epochs=10)\n",
    "    lda_model.save('/workspace/data/data_dir/saved/saved_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7b716b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1750 images belonging to 3 classes.\n",
      "Found 249 images belonging to 3 classes.\n",
      "Found 502 images belonging to 3 classes.\n",
      "Shape of lda_train_generator[0]: (32, 1)\n",
      "Shape of lda_train_generator[1]: (32,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 130>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     lda_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/data/data_dir/saved/saved_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mmodel_trainer\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of lda_train_generator[0]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lda_train_generator[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of lda_train_generator[1]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lda_train_generator[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 86\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[43mlda_train_generator\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Get number of classes from the generator\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes)\n\u001b[1;32m     89\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (lda_train_generator[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)  \u001b[38;5;66;03m# Adjust input shape according to LDA components\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess(train_data_dir, valid_data_dir, test_data_dir):\n",
    "    img_height, img_width = (227, 227)  # Image dimensions assumed for AlexNet\n",
    "    batch_size = 32\n",
    "\n",
    "    # Data generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    test_generator = train_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # Preprocess images using LDA\n",
    "    lda = LDA()\n",
    "    lda.fit(train_generator[0][0].reshape(-1, img_height * img_width * 3), np.argmax(train_generator[0][1], axis=1))\n",
    "\n",
    "    # Transform image data using LDA\n",
    "    lda_train_data = lda.transform(train_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "    lda_valid_data = lda.transform(valid_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "    lda_test_data = lda.transform(test_generator[0][0].reshape(-1, img_height * img_width * 3))\n",
    "\n",
    "    # Get the target variables\n",
    "    lda_train_target = np.argmax(train_generator[0][1], axis=1)\n",
    "    lda_valid_target = np.argmax(valid_generator[0][1], axis=1)\n",
    "    lda_test_target = np.argmax(test_generator[0][1], axis=1)\n",
    "\n",
    "    # Create new generators with transformed data and target variables\n",
    "    lda_train_generator = (lda_train_data, lda_train_target)\n",
    "    lda_valid_generator = (lda_valid_data, lda_valid_target)\n",
    "    lda_test_generator = (lda_test_data, lda_test_target)\n",
    "\n",
    "    return lda, lda_train_generator, lda_test_generator, lda_valid_generator\n",
    "\n",
    "def lda_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Flatten layer\n",
    "    x = Flatten()(input_layer)\n",
    "\n",
    "    # Dense layers\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "\n",
    "    # Output layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_trainer(epochs=10):\n",
    "    train_data_dir = \"/workspace/data/data_dir/model-data/train\"\n",
    "    test_data_dir = \"/workspace/data/data_dir/model-data/test\"\n",
    "    valid_data_dir = \"/workspace/data/data_dir/model-data/val\"\n",
    "\n",
    "    lda, lda_train_generator, lda_test_generator, lda_valid_generator = preprocess(\n",
    "        train_data_dir=train_data_dir,\n",
    "        test_data_dir=test_data_dir,\n",
    "        valid_data_dir=valid_data_dir)\n",
    "\n",
    "    print(\"Shape of lda_train_generator[0]:\", lda_train_generator[0].shape)\n",
    "    print(\"Shape of lda_train_generator[1]:\", lda_train_generator[1].shape)\n",
    "\n",
    "    num_classes = lda_train_generator[1].shape[1]  # Get number of classes from the generator\n",
    "    print(\"Number of classes:\", num_classes)\n",
    "\n",
    "    input_shape = (lda_train_generator[0].shape[1],)  # Adjust input shape according to LDA components\n",
    "    print(\"Input shape:\", input_shape)\n",
    "\n",
    "    model = lda_model(input_shape, num_classes)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint('/workspace/data/data_dir/saved/best/best_model.h5',\n",
    "                                          monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "    \n",
    "    log_dir = \"/workspace/data/data_dir/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    history = model.fit(\n",
    "        lda_train_generator[0],\n",
    "        lda_train_generator[1],\n",
    "        epochs=epochs,\n",
    "        validation_data=(lda_valid_generator[0], lda_valid_generator[1]),\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback, tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lda_model = model_trainer(epochs=10)\n",
    "    lda_model.save('/workspace/data/data_dir/saved/saved_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b088b0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
